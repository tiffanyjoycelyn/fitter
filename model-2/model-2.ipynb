{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from keras.utils import img_to_array, load_img\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.DataFrame(columns=['imagePath', 'label'])\n",
    "\n",
    "directories = {\n",
    "    \"ayam_goreng\": \"./dataset/ayam_goreng/\",\n",
    "    \"ayam_pop\": \"./dataset/ayam_pop/\",\n",
    "    \"daging_rendang\": \"./dataset/daging_rendang/\",\n",
    "    \"dendeng_batokok\": \"./dataset/dendeng_batokok/\",\n",
    "    \"gulai_ikan\": \"./dataset/gulai_ikan/\",\n",
    "    \"gulai_tambusu\": \"./dataset/gulai_tambusu/\",\n",
    "    \"telur_balado\": \"./dataset/telur_balado/\",\n",
    "    \"telur_dadar\": \"./dataset/telur_dadar/\",\n",
    "    \"tahu\": \"./dataset/tahu/\",\n",
    "    \"daun_singkong\": \"./dataset/daun_singkong/\",\n",
    "    \"nangka\": \"./dataset/nangka/\",\n",
    "    \"perkedel\": \"./dataset/perkedel/\",\n",
    "    \"nasi\" : \"./dataset/nasi/\"\n",
    "}\n",
    "\n",
    "for label, directory in directories.items():\n",
    "    for i in os.listdir(directory):\n",
    "        df = pd.concat([df, pd.DataFrame({'imagePath': [f\"{directory}/{i}\"], 'label': [label]})])\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for image_path in df['imagePath']:\n",
    "    img = load_img(image_path, target_size=(127, 127)).convert('RGB')\n",
    "    img_array = img_to_array(img) / 128\n",
    "    imgs.append(img_array)\n",
    "\n",
    "df['img'] = imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "model2 = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(127, 127, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(directories), activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer=Adam(learning_rate=01.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def cutout(image, mask_size=20, mask_value=0):\n",
    "    h, w, _ = image.shape\n",
    "    top = np.random.randint(0 - mask_size // 2, h - mask_size)\n",
    "    left = np.random.randint(0 - mask_size // 2, w - mask_size)\n",
    "    bottom = top + mask_size\n",
    "    right = left + mask_size\n",
    "    image[max(0, top):min(h, bottom), max(0, left):min(w, right), :] = mask_value\n",
    "    return image\n",
    "\n",
    "# Define a new preprocessing function that applies the cutout technique\n",
    "def augment_image(image):\n",
    "    image = image + np.random.normal(0, 0.05, image.shape)  # Gaussian noise\n",
    "    image = cutout(image)  # Cutout\n",
    "    return image\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.25,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest',\n",
    "     preprocessing_function=augment_image\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "x = np.array(df['img'].tolist())\n",
    "y = le.fit_transform(df['label'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "34/34 [==============================] - 22s 311ms/step - loss: 819334.6875 - accuracy: 0.0866 - val_loss: 59025633280.0000 - val_accuracy: 0.0654\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 7s 203ms/step - loss: 2.6589 - accuracy: 0.0702 - val_loss: 7289560064.0000 - val_accuracy: 0.0617\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 7s 193ms/step - loss: 2.6501 - accuracy: 0.0824 - val_loss: 316901664.0000 - val_accuracy: 0.1121\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 6s 186ms/step - loss: 23211.7754 - accuracy: 0.0786 - val_loss: 96314048512.0000 - val_accuracy: 0.1084\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 6s 184ms/step - loss: 2.6366 - accuracy: 0.0847 - val_loss: 7826061312.0000 - val_accuracy: 0.0804\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 7s 189ms/step - loss: 2.6335 - accuracy: 0.0744 - val_loss: 513642304.0000 - val_accuracy: 0.0636\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 6s 183ms/step - loss: 2.6235 - accuracy: 0.0927 - val_loss: 168556272.0000 - val_accuracy: 0.0430\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 6s 184ms/step - loss: 2.6299 - accuracy: 0.0753 - val_loss: 68103872.0000 - val_accuracy: 0.0748\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 7s 187ms/step - loss: 2.6226 - accuracy: 0.0711 - val_loss: 32957732.0000 - val_accuracy: 0.0748\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 7s 189ms/step - loss: 2.6552 - accuracy: 0.0688 - val_loss: 24195622.0000 - val_accuracy: 0.0860\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 7s 189ms/step - loss: 2.6260 - accuracy: 0.0767 - val_loss: 21538474.0000 - val_accuracy: 0.0654\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 6s 184ms/step - loss: 2.6352 - accuracy: 0.0669 - val_loss: 19268754.0000 - val_accuracy: 0.0654\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 6s 181ms/step - loss: 2.6551 - accuracy: 0.0707 - val_loss: 19674328.0000 - val_accuracy: 0.0131\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 7s 189ms/step - loss: 2.6616 - accuracy: 0.0824 - val_loss: 21415670.0000 - val_accuracy: 0.0879\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 6s 184ms/step - loss: 2.6608 - accuracy: 0.0791 - val_loss: 23559164.0000 - val_accuracy: 0.0804\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 6s 184ms/step - loss: 2.6362 - accuracy: 0.0707 - val_loss: 29616328.0000 - val_accuracy: 0.0897\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 7s 194ms/step - loss: 2.6395 - accuracy: 0.0819 - val_loss: 38251216.0000 - val_accuracy: 0.0804\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 19268754.0000 - accuracy: 0.0654\n",
      "Test Accuracy: 6.54%\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model2.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=50,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "loss, accuracy = model2.evaluate(x_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
